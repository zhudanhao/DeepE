0 train loss: 10.164464
1 train loss: 8.835723
2 train loss: 8.691965
3 train loss: 8.530895
4 train loss: 8.460685
5 train loss: 8.430898
6 train loss: 8.40301
7 train loss: 8.3874855
8 train loss: 8.368042
9 train loss: 8.337452
10 train loss: 8.3203
valid {'mr': 3015.711120615911, 'mrr': 0.0631191264826893, 'hits@1': 0.03082406615340747, 'hits@10': 0.12235528942115768}
test {'mr': 3030.5923238541973, 'mrr': 0.06296497451921386, 'hits@1': 0.03154011531320238, 'hits@10': 0.12024821655428515}
11 train loss: 8.312088
12 train loss: 8.304614
13 train loss: 8.29862
14 train loss: 8.294752
15 train loss: 8.288156
16 train loss: 8.258227
17 train loss: 8.243933
18 train loss: 8.236458
19 train loss: 8.231254
20 train loss: 8.19361
valid {'mr': 2285.8789563729683, 'mrr': 0.07511289992685444, 'hits@1': 0.03701169090390647, 'hits@10': 0.14471057884231536}
test {'mr': 2318.8331378872276, 'mrr': 0.07420855065957452, 'hits@1': 0.03708589856347112, 'hits@10': 0.1405501807876478}
21 train loss: 7.1965904
22 train loss: 6.6435347
23 train loss: 6.2571526
24 train loss: 6.02509
25 train loss: 5.8336325
26 train loss: 5.69319
27 train loss: 5.5816655
28 train loss: 5.473703
29 train loss: 5.3697844
30 train loss: 5.283666
valid {'mr': 243.58491588252068, 'mrr': 0.25806281112209084, 'hits@1': 0.18146564014827488, 'hits@10': 0.40824066153407473}
test {'mr': 256.3772842763608, 'mrr': 0.2560898764398833, 'hits@1': 0.1796638326981335, 'hits@10': 0.40601485390403597}
31 train loss: 5.2145066
32 train loss: 5.1489277
33 train loss: 5.0918374
34 train loss: 5.0429945
35 train loss: 4.9981775
36 train loss: 4.957453
37 train loss: 4.9225802
38 train loss: 4.887792
39 train loss: 4.859995
40 train loss: 4.828696
valid {'mr': 205.74254348445965, 'mrr': 0.29341908712172227, 'hits@1': 0.21211861990305103, 'hits@10': 0.4573424579412603}
test {'mr': 216.38195055213524, 'mrr': 0.29135584673939025, 'hits@1': 0.20993354832404965, 'hits@10': 0.45578031857715234}
41 train loss: 4.802196
42 train loss: 4.779079
43 train loss: 4.7518897
44 train loss: 4.7306533
45 train loss: 4.708506
46 train loss: 4.690359
47 train loss: 4.671261
48 train loss: 4.6512113
49 train loss: 4.6334968
50 train loss: 4.6169906
valid {'mr': 185.14456800684346, 'mrr': 0.3118392292684081, 'hits@1': 0.22680353578557172, 'hits@10': 0.48120901055032794}
test {'mr': 196.73973907944884, 'mrr': 0.30833702808381874, 'hits@1': 0.22393237564741522, 'hits@10': 0.4780856053943125}
51 train loss: 4.6008
52 train loss: 4.586332
53 train loss: 4.5710077
54 train loss: 4.558017
55 train loss: 4.545094
56 train loss: 4.531332
57 train loss: 4.5200458
58 train loss: 4.507071
59 train loss: 4.4967484
60 train loss: 4.487253
valid {'mr': 174.37530652979754, 'mrr': 0.3207024036721628, 'hits@1': 0.2352437981180496, 'hits@10': 0.49147419446820645}
test {'mr': 188.30440730968436, 'mrr': 0.3184686477222322, 'hits@1': 0.23265415811589954, 'hits@10': 0.49218215577054625}
61 train loss: 4.4763627
62 train loss: 4.4650416
63 train loss: 4.45487
64 train loss: 4.4474893
65 train loss: 4.4377923
66 train loss: 4.4293866
67 train loss: 4.422048
68 train loss: 4.4131784
69 train loss: 4.4072013
70 train loss: 4.3970737
valid {'mr': 166.84414029084687, 'mrr': 0.32690274836869054, 'hits@1': 0.24043341887653266, 'hits@10': 0.49669232962646137}
test {'mr': 180.2978842959054, 'mrr': 0.32332400666667505, 'hits@1': 0.2358301573341151, 'hits@10': 0.49655526238639697}
71 train loss: 4.3905163
72 train loss: 4.3847237
73 train loss: 4.376533
74 train loss: 4.367734
75 train loss: 4.3616853
76 train loss: 4.355095
77 train loss: 4.3493047
78 train loss: 4.3444257
79 train loss: 4.337665
80 train loss: 4.329555
valid {'mr': 163.84140290846878, 'mrr': 0.3292473382136621, 'hits@1': 0.24254348445965213, 'hits@10': 0.5028514399771885}
test {'mr': 177.2819554382879, 'mrr': 0.3268175511836792, 'hits@1': 0.23966578715919085, 'hits@10': 0.5014169842665885}
81 train loss: 4.326009
82 train loss: 4.317314
83 train loss: 4.312896
84 train loss: 4.307879
85 train loss: 4.303825
86 train loss: 4.298508
87 train loss: 4.2925496
88 train loss: 4.2878623
89 train loss: 4.2837777
90 train loss: 4.2803855
valid {'mr': 159.44479612204162, 'mrr': 0.33439857618566626, 'hits@1': 0.24699173082406614, 'hits@10': 0.5082406615340747}
test {'mr': 173.03447180689923, 'mrr': 0.33187667430457046, 'hits@1': 0.2433304016417473, 'hits@10': 0.5090393823903059}
91 train loss: 4.2753153
92 train loss: 4.2703547
93 train loss: 4.266303
94 train loss: 4.2649326
95 train loss: 4.2568994
96 train loss: 4.2558217
97 train loss: 4.249835
98 train loss: 4.2453985
99 train loss: 4.2436233
100 train loss: 4.2371573
valid {'mr': 157.66792130025664, 'mrr': 0.3380754759563527, 'hits@1': 0.25129740518962074, 'hits@10': 0.5106928999144568}
test {'mr': 171.87992279878824, 'mrr': 0.33334083125676445, 'hits@1': 0.2451627088830255, 'hits@10': 0.5094302746017786}
101 train loss: 4.233962
102 train loss: 4.231183
103 train loss: 4.2280774
104 train loss: 4.2226176
105 train loss: 4.2227488
106 train loss: 4.2160387
107 train loss: 4.2147336
108 train loss: 4.2090244
109 train loss: 4.2073073
110 train loss: 4.202923
valid {'mr': 154.22563444539492, 'mrr': 0.34016108295057007, 'hits@1': 0.25209580838323353, 'hits@10': 0.5158540062731679}
test {'mr': 169.2968826346135, 'mrr': 0.33646423407913795, 'hits@1': 0.24780123131046614, 'hits@10': 0.5139011042704974}
111 train loss: 4.201721
112 train loss: 4.1972756
113 train loss: 4.192676
114 train loss: 4.1932535
115 train loss: 4.188235
116 train loss: 4.1850057
117 train loss: 4.183508
118 train loss: 4.1819453
119 train loss: 4.1786985
120 train loss: 4.174952
valid {'mr': 154.39090390647277, 'mrr': 0.3404687189598597, 'hits@1': 0.25229540918163673, 'hits@10': 0.5157114342743085}
test {'mr': 167.99907163099775, 'mrr': 0.33775707843792374, 'hits@1': 0.24919378481383758, 'hits@10': 0.5157822730382097}
121 train loss: 4.171558
122 train loss: 4.1718316
123 train loss: 4.1652546
124 train loss: 4.1640253
125 train loss: 4.162091
126 train loss: 4.160093
127 train loss: 4.156464
128 train loss: 4.154734
129 train loss: 4.1527424
130 train loss: 4.1498995
valid {'mr': 152.82437981180496, 'mrr': 0.34038212707955245, 'hits@1': 0.25258055317935557, 'hits@10': 0.5166238950670088}
test {'mr': 167.45536499560245, 'mrr': 0.3392008003872143, 'hits@1': 0.25122153816085213, 'hits@10': 0.5164663344082869}
131 train loss: 4.145134
132 train loss: 4.145782
133 train loss: 4.144257
134 train loss: 4.140254
135 train loss: 4.138175
136 train loss: 4.1358967
137 train loss: 4.135302
138 train loss: 4.1324325
139 train loss: 4.1305428
140 train loss: 4.128884
valid {'mr': 151.58619903051041, 'mrr': 0.3426346854441631, 'hits@1': 0.25315084117479325, 'hits@10': 0.5201881950384944}
test {'mr': 165.27245187139647, 'mrr': 0.34088293735387215, 'hits@1': 0.2512459689240692, 'hits@10': 0.5191537183621616}
141 train loss: 4.1266575
142 train loss: 4.1252146
143 train loss: 4.1230216
144 train loss: 4.11856
145 train loss: 4.119471
146 train loss: 4.1163373
147 train loss: 4.1158485
148 train loss: 4.111849
149 train loss: 4.111543
150 train loss: 4.107739
valid {'mr': 151.24952951240377, 'mrr': 0.34489345982934544, 'hits@1': 0.2570287995437696, 'hits@10': 0.5213572854291417}
test {'mr': 164.63744747385908, 'mrr': 0.3412331341983265, 'hits@1': 0.2516368611355419, 'hits@10': 0.5206684256816183}
151 train loss: 4.1042147
152 train loss: 4.104008
153 train loss: 4.10248
154 train loss: 4.101464
155 train loss: 4.100212
156 train loss: 4.097774
157 train loss: 4.0967627
158 train loss: 4.0961695
159 train loss: 4.0918417
160 train loss: 4.0917807
valid {'mr': 150.55340747077275, 'mrr': 0.34290084350651995, 'hits@1': 0.2527516395779869, 'hits@10': 0.523011120615911}
test {'mr': 165.81286035375746, 'mrr': 0.3420632345093817, 'hits@1': 0.2523453532688361, 'hits@10': 0.5208638717873546}
161 train loss: 4.091805
162 train loss: 4.0894885
163 train loss: 4.084782
164 train loss: 4.083481
165 train loss: 4.0844855
166 train loss: 4.0824037
167 train loss: 4.079333
168 train loss: 4.077961
169 train loss: 4.078317
170 train loss: 4.0744395
valid {'mr': 148.6416880524665, 'mrr': 0.3480759052437067, 'hits@1': 0.2585115483319076, 'hits@10': 0.5254348445965212}
test {'mr': 163.09330108472588, 'mrr': 0.34627026042120834, 'hits@1': 0.2555213524870517, 'hits@10': 0.5260920551158018}
171 train loss: 4.0744815
172 train loss: 4.075025
173 train loss: 4.0717154
174 train loss: 4.068393
175 train loss: 4.0688915
176 train loss: 4.0663724
177 train loss: 4.0664206
178 train loss: 4.0643387
179 train loss: 4.06412
180 train loss: 4.0631514
valid {'mr': 146.64599372683205, 'mrr': 0.34688260130931414, 'hits@1': 0.25868263473053893, 'hits@10': 0.5226974622184203}
test {'mr': 161.56501026092056, 'mrr': 0.3439697839842173, 'hits@1': 0.2546662757744552, 'hits@10': 0.5236001172676634}
181 train loss: 4.061019
182 train loss: 4.058966
183 train loss: 4.057851
184 train loss: 4.0554724
185 train loss: 4.0563836
186 train loss: 4.052977
187 train loss: 4.054039
188 train loss: 4.053648
189 train loss: 4.0480237
190 train loss: 4.0484633
valid {'mr': 147.07299686341602, 'mrr': 0.34634447344430785, 'hits@1': 0.2564870259481038, 'hits@10': 0.5253493013972056}
test {'mr': 162.34591517639012, 'mrr': 0.34522641881505584, 'hits@1': 0.25451969119515294, 'hits@10': 0.5249926707710348}
191 train loss: 4.046281
192 train loss: 4.046075
193 train loss: 4.0449004
194 train loss: 4.0423846
195 train loss: 4.0427523
196 train loss: 4.0413284
197 train loss: 4.040046
198 train loss: 4.0387535
199 train loss: 4.037156
200 train loss: 4.0365353
valid {'mr': 148.80732820074138, 'mrr': 0.34908339876425276, 'hits@1': 0.2602224123182207, 'hits@10': 0.5278585685771314}
test {'mr': 162.36133098798007, 'mrr': 0.34660010882195297, 'hits@1': 0.25662073683181863, 'hits@10': 0.5262386396951041}
201 train loss: 4.037175
202 train loss: 4.0331573
203 train loss: 4.0334363
204 train loss: 4.034102
205 train loss: 4.030899
206 train loss: 4.028651
207 train loss: 4.0295753
208 train loss: 4.0271726
209 train loss: 4.0236783
210 train loss: 4.0247955
valid {'mr': 146.62925577416596, 'mrr': 0.3479061143093478, 'hits@1': 0.25757057313943543, 'hits@10': 0.528970630168235}
test {'mr': 160.47102511482458, 'mrr': 0.34595684964216034, 'hits@1': 0.25442196814228474, 'hits@10': 0.5275579009088244}
211 train loss: 4.025925
212 train loss: 4.023758
213 train loss: 4.0201044
214 train loss: 4.020402
215 train loss: 4.0197353
216 train loss: 4.0177298
217 train loss: 4.0177917
218 train loss: 4.0191383
219 train loss: 4.015563
220 train loss: 4.0158286
valid {'mr': 146.83746792130026, 'mrr': 0.34892076218694384, 'hits@1': 0.25896777872825777, 'hits@10': 0.5281722269746222}
test {'mr': 161.40210593178932, 'mrr': 0.3457305535753051, 'hits@1': 0.2542998143261995, 'hits@10': 0.5299521157040946}
221 train loss: 4.0144057
222 train loss: 4.013887
223 train loss: 4.012391
224 train loss: 4.0089707
225 train loss: 4.0090632
226 train loss: 4.0070906
227 train loss: 4.0083623
228 train loss: 4.00769
229 train loss: 4.0054154
230 train loss: 4.0033617
valid {'mr': 145.97177074422584, 'mrr': 0.34875427516733576, 'hits@1': 0.25933846592529225, 'hits@10': 0.5286569717707442}
test {'mr': 162.331720902961, 'mrr': 0.34706794189918194, 'hits@1': 0.25588781393530735, 'hits@10': 0.5294635004397538}
231 train loss: 4.001674
232 train loss: 4.002842
233 train loss: 4.0025783
234 train loss: 4.0021343
235 train loss: 3.9982104
236 train loss: 4.0010815
237 train loss: 3.9981518
238 train loss: 3.9985697
239 train loss: 4.000493
240 train loss: 3.9986584
valid {'mr': 144.5966353008269, 'mrr': 0.35037914755871813, 'hits@1': 0.2607071571143427, 'hits@10': 0.5302252637581979}
test {'mr': 160.50598553698816, 'mrr': 0.3480657721149533, 'hits@1': 0.2565230137789505, 'hits@10': 0.5308560539431252}
241 train loss: 3.9969761
242 train loss: 3.994705
243 train loss: 3.9959254
244 train loss: 3.9934337
245 train loss: 3.9918292
246 train loss: 3.991008
247 train loss: 3.9916804
248 train loss: 3.9921768
249 train loss: 3.9874587
250 train loss: 3.9870834
valid {'mr': 143.88502994011975, 'mrr': 0.3512924244360097, 'hits@1': 0.26199030510407756, 'hits@10': 0.5309666381522669}
test {'mr': 160.42653669500635, 'mrr': 0.349201268121607, 'hits@1': 0.2582575979673605, 'hits@10': 0.5307339001270399}
251 train loss: 3.9862354
252 train loss: 3.985721
253 train loss: 3.9893556
254 train loss: 3.986081
255 train loss: 3.984324
256 train loss: 3.9838374
257 train loss: 3.984475
258 train loss: 3.9813352
259 train loss: 3.9840972
260 train loss: 3.9786146
valid {'mr': 144.86344453949243, 'mrr': 0.3499994950411681, 'hits@1': 0.25988023952095807, 'hits@10': 0.5286854861705161}
test {'mr': 160.86565523306948, 'mrr': 0.34890021311899105, 'hits@1': 0.2575246750708492, 'hits@10': 0.5311003615752956}
261 train loss: 3.982798
262 train loss: 3.9796455
263 train loss: 3.979597
264 train loss: 3.9768403
265 train loss: 3.975448
266 train loss: 3.9755712
267 train loss: 3.974617
268 train loss: 3.9728255
269 train loss: 3.976194
270 train loss: 3.9743934
valid {'mr': 143.2940119760479, 'mrr': 0.35131667841355035, 'hits@1': 0.26184773310521814, 'hits@10': 0.5305674365554605}
test {'mr': 160.7524186455585, 'mrr': 0.34738650909724184, 'hits@1': 0.2559611062249585, 'hits@10': 0.5299276849408776}
271 train loss: 3.9720004
272 train loss: 3.9735558
273 train loss: 3.9713411
274 train loss: 3.9688718
275 train loss: 3.9676669
276 train loss: 3.9672694
277 train loss: 3.9685135
278 train loss: 3.9671078
279 train loss: 3.964255
280 train loss: 3.9670308
valid {'mr': 141.36455660108354, 'mrr': 0.35166540534278595, 'hits@1': 0.26218990590248076, 'hits@10': 0.5294838893641289}
test {'mr': 159.3940682106909, 'mrr': 0.34824160892060113, 'hits@1': 0.2568894752272061, 'hits@10': 0.5318088537085899}
281 train loss: 3.965068
282 train loss: 3.9651852
283 train loss: 3.9627066
284 train loss: 3.9620466
285 train loss: 3.9648707
286 train loss: 3.9637403
287 train loss: 3.9604287
288 train loss: 3.9599931
289 train loss: 3.9595659
290 train loss: 3.9592414
valid {'mr': 143.26238950670088, 'mrr': 0.3513329548035836, 'hits@1': 0.2615055603079555, 'hits@10': 0.5312517821499857}
test {'mr': 161.78061174631097, 'mrr': 0.3490806310358267, 'hits@1': 0.25703605980650834, 'hits@10': 0.5316378383660706}
291 train loss: 3.957618
292 train loss: 3.9576776
293 train loss: 3.9572544
294 train loss: 3.9550006
295 train loss: 3.9570854
296 train loss: 3.9557908
297 train loss: 3.9554982
298 train loss: 3.9550154
299 train loss: 3.953993
300 train loss: 3.9531107
valid {'mr': 143.75195323638437, 'mrr': 0.35117462968139285, 'hits@1': 0.2603649843170801, 'hits@10': 0.5328485885372113}
test {'mr': 161.030953776996, 'mrr': 0.3499759493247433, 'hits@1': 0.2579155672823219, 'hits@10': 0.5327127919476204}
301 train loss: 3.9524763
302 train loss: 3.9531944
303 train loss: 3.9512162
304 train loss: 3.948997
305 train loss: 3.9488094
306 train loss: 3.9491234
307 train loss: 3.9484222
308 train loss: 3.9504914
309 train loss: 3.947865
310 train loss: 3.946238
valid {'mr': 142.28942115768464, 'mrr': 0.3528447456053812, 'hits@1': 0.26295979469632164, 'hits@10': 0.5314228685486171}
test {'mr': 160.77885273135934, 'mrr': 0.350955307622831, 'hits@1': 0.2591615362063911, 'hits@10': 0.5349359914003714}
311 train loss: 3.9467533
312 train loss: 3.9469466
313 train loss: 3.9463732
314 train loss: 3.944607
315 train loss: 3.9432652
316 train loss: 3.9426177
317 train loss: 3.9425907
318 train loss: 3.9420347
319 train loss: 3.9418292
320 train loss: 3.9414349
valid {'mr': 143.918876532649, 'mrr': 0.3530038649463467, 'hits@1': 0.26238950670088396, 'hits@10': 0.534388366124893}
test {'mr': 161.7103000097723, 'mrr': 0.35122421479007987, 'hits@1': 0.26006547444542166, 'hits@10': 0.5348382683475031}
321 train loss: 3.9413543
322 train loss: 3.9392183
323 train loss: 3.9413104
324 train loss: 3.937894
325 train loss: 3.9388847
326 train loss: 3.9354842
327 train loss: 3.9355423
328 train loss: 3.9357421
329 train loss: 3.937006
330 train loss: 3.9373217
valid {'mr': 143.7080980895352, 'mrr': 0.3516457551894548, 'hits@1': 0.25996578272027376, 'hits@10': 0.5337040205303678}
test {'mr': 160.04952115704094, 'mrr': 0.34927356719597963, 'hits@1': 0.25662073683181863, 'hits@10': 0.5345206684256816}
331 train loss: 3.9365659
332 train loss: 3.9348114
333 train loss: 3.93211
334 train loss: 3.934693
335 train loss: 3.9321182
336 train loss: 3.933798
337 train loss: 3.9302828
338 train loss: 3.9320903
339 train loss: 3.9312348
340 train loss: 3.9300253
valid {'mr': 143.79153122326775, 'mrr': 0.3544310916025033, 'hits@1': 0.26444254348445967, 'hits@10': 0.5336184773310522}
test {'mr': 160.32202188996385, 'mrr': 0.3500375500912752, 'hits@1': 0.2582575979673605, 'hits@10': 0.5343740838463794}
341 train loss: 3.9318135
342 train loss: 3.9291043
343 train loss: 3.929958
344 train loss: 3.9279804
345 train loss: 3.928951
346 train loss: 3.9283078
347 train loss: 3.927639
348 train loss: 3.9265082
349 train loss: 3.926081
350 train loss: 3.9263694
valid {'mr': 143.1600228115198, 'mrr': 0.3521572214525362, 'hits@1': 0.26087824351297406, 'hits@10': 0.5325919589392644}
test {'mr': 161.02743574709274, 'mrr': 0.3495398811332768, 'hits@1': 0.2575246750708492, 'hits@10': 0.5337633147659533}
351 train loss: 3.9263206
352 train loss: 3.926869
353 train loss: 3.9236634
354 train loss: 3.9223766
355 train loss: 3.9246068
356 train loss: 3.922434
357 train loss: 3.9224107
358 train loss: 3.9209077
359 train loss: 3.9196243
360 train loss: 3.9185512
valid {'mr': 143.8014257199886, 'mrr': 0.3539277702100025, 'hits@1': 0.2631879098944967, 'hits@10': 0.5333048189335614}
test {'mr': 162.38935307339003, 'mrr': 0.3516630685795328, 'hits@1': 0.2594058438385615, 'hits@10': 0.5362308218508747}
361 train loss: 3.9189122
362 train loss: 3.9181623
363 train loss: 3.9210794
364 train loss: 3.919886
365 train loss: 3.919672
366 train loss: 3.9176645
367 train loss: 3.9169524
368 train loss: 3.9175477
369 train loss: 3.9164636
370 train loss: 3.9174483
valid {'mr': 145.75337895637296, 'mrr': 0.35434571321484026, 'hits@1': 0.2637867122897063, 'hits@10': 0.5323923581408612}
test {'mr': 161.6628310368416, 'mrr': 0.3523059656665594, 'hits@1': 0.260260920551158, 'hits@10': 0.5356200527704486}
371 train loss: 3.9149127
372 train loss: 3.9165354
373 train loss: 3.9157093
374 train loss: 3.9138918
375 train loss: 3.9141119
376 train loss: 3.913879
377 train loss: 3.912329
378 train loss: 3.9117296
379 train loss: 3.9150445
380 train loss: 3.9115534
valid {'mr': 144.41402908468777, 'mrr': 0.3543172854465112, 'hits@1': 0.26429997148560025, 'hits@10': 0.533675506130596}
test {'mr': 163.07160656698915, 'mrr': 0.3524953774105216, 'hits@1': 0.26131144336949086, 'hits@10': 0.5343007915567283}
381 train loss: 3.9122894
382 train loss: 3.911191
383 train loss: 3.9104004
384 train loss: 3.9112923
385 train loss: 3.9089162
386 train loss: 3.9092221
387 train loss: 3.907671
388 train loss: 3.9070747
389 train loss: 3.9064767
390 train loss: 3.9077256
valid {'mr': 143.0617621899059, 'mrr': 0.35403493158209287, 'hits@1': 0.2634160250926718, 'hits@10': 0.5353578557171371}
test {'mr': 161.86924655526238, 'mrr': 0.35214447379310554, 'hits@1': 0.26060295123619664, 'hits@10': 0.5353757451382781}
391 train loss: 3.9089038
392 train loss: 3.9070294
393 train loss: 3.9070842
394 train loss: 3.9056308
395 train loss: 3.9052868
396 train loss: 3.9038947
397 train loss: 3.9036043
398 train loss: 3.90812
399 train loss: 3.9034803
400 train loss: 3.9006772
valid {'mr': 143.57781579697746, 'mrr': 0.3562401219605977, 'hits@1': 0.2658967778728258, 'hits@10': 0.5348731109210151}
test {'mr': 162.56066158506792, 'mrr': 0.35204221065653324, 'hits@1': 0.25891722857422067, 'hits@10': 0.5370614678002541}
401 train loss: 3.9023106
402 train loss: 3.9021404
403 train loss: 3.9018376
404 train loss: 3.9020445
405 train loss: 3.9005706
406 train loss: 3.903278
Epoch 00407: reducing learning rate of group 0 to 2.4000e-03.
407 train loss: 3.885925
408 train loss: 3.8817267
409 train loss: 3.879288
410 train loss: 3.8765957
valid {'mr': 144.35166809238666, 'mrr': 0.35509732781439834, 'hits@1': 0.26427145708582833, 'hits@10': 0.5357285429141716}
test {'mr': 161.9494038893775, 'mrr': 0.3521028883786857, 'hits@1': 0.25938141307534446, 'hits@10': 0.5361330987980064}
411 train loss: 3.8757768
412 train loss: 3.875553
413 train loss: 3.8725877
414 train loss: 3.8732493
415 train loss: 3.8725066
416 train loss: 3.872212
417 train loss: 3.870686
418 train loss: 3.8683033
419 train loss: 3.8695967
420 train loss: 3.868897
valid {'mr': 143.9688622754491, 'mrr': 0.3560325693600207, 'hits@1': 0.2652694610778443, 'hits@10': 0.5363558597091531}
test {'mr': 161.57216847454313, 'mrr': 0.3528852063726303, 'hits@1': 0.25938141307534446, 'hits@10': 0.5383807290139744}
421 train loss: 3.868345
422 train loss: 3.8680727
423 train loss: 3.8669035
424 train loss: 3.8690977
425 train loss: 3.8654091
426 train loss: 3.8655286
427 train loss: 3.8619123
428 train loss: 3.8628244
429 train loss: 3.8633063
430 train loss: 3.8647738
valid {'mr': 144.41819218705447, 'mrr': 0.3566230773448261, 'hits@1': 0.2662389506700884, 'hits@10': 0.5377530652979755}
test {'mr': 161.17184598846868, 'mrr': 0.3538601747884263, 'hits@1': 0.2618244893970488, 'hits@10': 0.5373057754324245}
431 train loss: 3.8605387
432 train loss: 3.8617613
433 train loss: 3.859795
434 train loss: 3.860726
435 train loss: 3.85806
436 train loss: 3.8596563
437 train loss: 3.8623612
438 train loss: 3.8611069
439 train loss: 3.8594835
440 train loss: 3.859978
valid {'mr': 145.15249500998004, 'mrr': 0.35510588850584135, 'hits@1': 0.2649843170801255, 'hits@10': 0.5340461933276305}
test {'mr': 163.2442343398808, 'mrr': 0.35293500605823513, 'hits@1': 0.2611159972637545, 'hits@10': 0.5344962376624646}
441 train loss: 3.858368
Epoch 00442: reducing learning rate of group 0 to 1.9200e-03.
442 train loss: 3.8473086
443 train loss: 3.8425486
444 train loss: 3.8410869
445 train loss: 3.8410068
446 train loss: 3.8392577
447 train loss: 3.835906
448 train loss: 3.8345227
449 train loss: 3.8364787
450 train loss: 3.8344197
valid {'mr': 144.24177359566582, 'mrr': 0.35758341135935456, 'hits@1': 0.2675506130595951, 'hits@10': 0.5366695181066439}
test {'mr': 162.17191928075832, 'mrr': 0.35390296264058535, 'hits@1': 0.26067624352584773, 'hits@10': 0.5379654060392847}
451 train loss: 3.833191
452 train loss: 3.8313065
453 train loss: 3.8323538
454 train loss: 3.8315918
455 train loss: 3.8308308
456 train loss: 3.8298845
457 train loss: 3.8277676
458 train loss: 3.8302014
459 train loss: 3.8280041
460 train loss: 3.8296185
valid {'mr': 144.6720273738238, 'mrr': 0.3576844175414954, 'hits@1': 0.2670088394639293, 'hits@10': 0.5398061020815512}
test {'mr': 161.99677513925536, 'mrr': 0.35387145667887054, 'hits@1': 0.2624108277142578, 'hits@10': 0.5378921137496335}
461 train loss: 3.8288176
462 train loss: 3.8275087
463 train loss: 3.826256
464 train loss: 3.8270526
465 train loss: 3.825759
466 train loss: 3.8274348
467 train loss: 3.8251953
468 train loss: 3.8266072
469 train loss: 3.825478
470 train loss: 3.8251407
valid {'mr': 144.6559737667522, 'mrr': 0.3560273840910834, 'hits@1': 0.26467065868263473, 'hits@10': 0.5384659252922726}
test {'mr': 161.5761262581843, 'mrr': 0.35392178198144353, 'hits@1': 0.2612381510798397, 'hits@10': 0.5378188214599824}
471 train loss: 3.8228211
472 train loss: 3.82376
473 train loss: 3.823441
474 train loss: 3.8219306
475 train loss: 3.8226707
476 train loss: 3.824001
477 train loss: 3.8217661
478 train loss: 3.823727
479 train loss: 3.8216524
480 train loss: 3.8223565
Epoch 00481: reducing learning rate of group 0 to 1.5360e-03.
valid {'mr': 143.4343883661249, 'mrr': 0.35852435470218313, 'hits@1': 0.26783575705731394, 'hits@10': 0.5402623324779013}
test {'mr': 159.51925144141504, 'mrr': 0.35479917860720106, 'hits@1': 0.26199550473956806, 'hits@10': 0.5399687286230822}
481 train loss: 3.8113012
482 train loss: 3.8096375
483 train loss: 3.8074958
484 train loss: 3.805931
485 train loss: 3.8066227
486 train loss: 3.8043005
487 train loss: 3.8040895
488 train loss: 3.8029404
489 train loss: 3.8018231
490 train loss: 3.8024144
valid {'mr': 145.9155403478757, 'mrr': 0.35755288184822565, 'hits@1': 0.26592529227259765, 'hits@10': 0.539834616481323}
test {'mr': 163.97481188312324, 'mrr': 0.3543471261216341, 'hits@1': 0.26145802794879314, 'hits@10': 0.5396755594644776}
491 train loss: 3.7997365
492 train loss: 3.799561
493 train loss: 3.7977583
494 train loss: 3.7984295
495 train loss: 3.7992446
496 train loss: 3.7987816
497 train loss: 3.796925
498 train loss: 3.7980442
499 train loss: 3.794576
500 train loss: 3.795292
valid {'mr': 144.59903051040774, 'mrr': 0.3579475732380255, 'hits@1': 0.26703735386370114, 'hits@10': 0.5396635300826917}
test {'mr': 161.38725202775333, 'mrr': 0.3541739614414366, 'hits@1': 0.2613358741327079, 'hits@10': 0.5401886054920356}
501 train loss: 3.79533
502 train loss: 3.7948754
503 train loss: 3.7978418
504 train loss: 3.7959745
505 train loss: 3.7941003
506 train loss: 3.796142
507 train loss: 3.795006
508 train loss: 3.7934866
509 train loss: 3.793026
510 train loss: 3.793062
valid {'mr': 145.05896777872826, 'mrr': 0.3576371994439108, 'hits@1': 0.2666096378671229, 'hits@10': 0.5385799828913601}
test {'mr': 163.24201114042802, 'mrr': 0.35463001514821035, 'hits@1': 0.26270399687286233, 'hits@10': 0.5393090980162221}
511 train loss: 3.7921884
512 train loss: 3.7908912
513 train loss: 3.7943225
514 train loss: 3.793673
515 train loss: 3.791977
516 train loss: 3.7922456
517 train loss: 3.791202
518 train loss: 3.7896812
519 train loss: 3.7901278
520 train loss: 3.7893653
valid {'mr': 146.23929284288565, 'mrr': 0.35870454902406423, 'hits@1': 0.26843455945252354, 'hits@10': 0.5405474764756202}
test {'mr': 162.732214404378, 'mrr': 0.35574748491818703, 'hits@1': 0.2634613505325906, 'hits@10': 0.5407260822828105}
521 train loss: 3.7892041
522 train loss: 3.7887888
523 train loss: 3.789251
524 train loss: 3.7887588
525 train loss: 3.7912986
526 train loss: 3.7907531
527 train loss: 3.7888021
528 train loss: 3.7896266
Epoch 00529: reducing learning rate of group 0 to 1.2288e-03.
529 train loss: 3.7799804
530 train loss: 3.7782612
valid {'mr': 145.78129455374963, 'mrr': 0.3579813272554939, 'hits@1': 0.26632449386940404, 'hits@10': 0.5405474764756202}
test {'mr': 162.55316134076028, 'mrr': 0.35521345354606754, 'hits@1': 0.26160461252809536, 'hits@10': 0.5407993745724616}
531 train loss: 3.7788212
532 train loss: 3.7748601
533 train loss: 3.7746046
534 train loss: 3.7738044
535 train loss: 3.7721653
536 train loss: 3.7737606
537 train loss: 3.7717643
538 train loss: 3.77191
539 train loss: 3.771438
540 train loss: 3.7718854
valid {'mr': 144.83398916452808, 'mrr': 0.35833817079798497, 'hits@1': 0.26672369546621044, 'hits@10': 0.5401197604790419}
test {'mr': 161.96684745431446, 'mrr': 0.3559168158894519, 'hits@1': 0.262532981530343, 'hits@10': 0.5422652203654842}
541 train loss: 3.7698157
542 train loss: 3.7703834
543 train loss: 3.7690215
544 train loss: 3.768501
545 train loss: 3.767498
546 train loss: 3.768854
547 train loss: 3.7687535
548 train loss: 3.7687922
549 train loss: 3.7682052
550 train loss: 3.767077
valid {'mr': 146.20815511833476, 'mrr': 0.3579505839742943, 'hits@1': 0.26646706586826346, 'hits@10': 0.5399771884801825}
test {'mr': 162.76082282810515, 'mrr': 0.3552877076229287, 'hits@1': 0.2616534740545295, 'hits@10': 0.5411169744942832}
551 train loss: 3.76652
552 train loss: 3.7652676
553 train loss: 3.7658274
554 train loss: 3.7665308
555 train loss: 3.7656908
556 train loss: 3.7638774
557 train loss: 3.7681127
558 train loss: 3.7645612
559 train loss: 3.7648635
560 train loss: 3.765823
valid {'mr': 145.1163102366695, 'mrr': 0.35917769574870595, 'hits@1': 0.26817792985457656, 'hits@10': 0.5407470772740234}
test {'mr': 163.48328935795953, 'mrr': 0.3566775595551924, 'hits@1': 0.26358350434867583, 'hits@10': 0.5418010358643603}
561 train loss: 3.7622402
562 train loss: 3.7621193
563 train loss: 3.7623253
564 train loss: 3.762095
565 train loss: 3.7638125
566 train loss: 3.7640316
567 train loss: 3.7623065
Epoch 00568: reducing learning rate of group 0 to 9.8304e-04.
568 train loss: 3.7550046
569 train loss: 3.7546744
570 train loss: 3.7517219
valid {'mr': 146.9163387510693, 'mrr': 0.3577769894270725, 'hits@1': 0.26595380667236956, 'hits@10': 0.5394924436840605}
test {'mr': 163.63483338219487, 'mrr': 0.3544622276584267, 'hits@1': 0.26067624352584773, 'hits@10': 0.5417033128114922}
571 train loss: 3.7526402
572 train loss: 3.7502282
573 train loss: 3.7486496
574 train loss: 3.7499018
575 train loss: 3.7483928
576 train loss: 3.7490914
577 train loss: 3.7499137
578 train loss: 3.7467897
579 train loss: 3.747779
580 train loss: 3.7504337
valid {'mr': 147.16213287710295, 'mrr': 0.3577581728288466, 'hits@1': 0.26612489307100085, 'hits@10': 0.5416880524664955}
test {'mr': 162.68186260138768, 'mrr': 0.35533421956979755, 'hits@1': 0.26204436626600214, 'hits@10': 0.5430470047884296}
581 train loss: 3.74716
582 train loss: 3.7502472
583 train loss: 3.7477617
584 train loss: 3.7478077
Epoch 00585: reducing learning rate of group 0 to 7.8643e-04.
585 train loss: 3.7424676
586 train loss: 3.7392452
587 train loss: 3.7389102
588 train loss: 3.736695
589 train loss: 3.7368362
590 train loss: 3.7348833
valid {'mr': 146.59304248645566, 'mrr': 0.3582038266284306, 'hits@1': 0.2667807242657542, 'hits@10': 0.5396920444824637}
test {'mr': 163.78000097723051, 'mrr': 0.3566028669435873, 'hits@1': 0.2642919964819701, 'hits@10': 0.5413612821264536}
591 train loss: 3.738031
592 train loss: 3.7354739
593 train loss: 3.7349098
594 train loss: 3.7353587
595 train loss: 3.7365417
596 train loss: 3.7314007
597 train loss: 3.7342706
598 train loss: 3.733891
599 train loss: 3.7324944
600 train loss: 3.7331674
valid {'mr': 147.31445680068435, 'mrr': 0.3597221959992916, 'hits@1': 0.26854861705161104, 'hits@10': 0.5409466780724266}
test {'mr': 163.97757255936676, 'mrr': 0.35714100147751193, 'hits@1': 0.26424313495553603, 'hits@10': 0.5433401739470342}
601 train loss: 3.7315636
602 train loss: 3.7333562
Epoch 00603: reducing learning rate of group 0 to 6.2915e-04.
603 train loss: 3.7274392
604 train loss: 3.72495
605 train loss: 3.7250996
606 train loss: 3.7244892
607 train loss: 3.7225606
608 train loss: 3.7256162
609 train loss: 3.723429
610 train loss: 3.7216465
valid {'mr': 147.0831765041346, 'mrr': 0.3589470748091907, 'hits@1': 0.2674080410607357, 'hits@10': 0.5405189620758483}
test {'mr': 164.07382976644192, 'mrr': 0.35589481271662626, 'hits@1': 0.26314375061076906, 'hits@10': 0.542704974103391}
611 train loss: 3.7226174
612 train loss: 3.721154
613 train loss: 3.721405
614 train loss: 3.7218387
615 train loss: 3.7194207
616 train loss: 3.71912
617 train loss: 3.7179086
618 train loss: 3.7199588
619 train loss: 3.7183096
620 train loss: 3.7202468
valid {'mr': 146.7237239806102, 'mrr': 0.3596175691538674, 'hits@1': 0.2682064442543484, 'hits@10': 0.5425149700598803}
test {'mr': 164.05914687774847, 'mrr': 0.3565156800551991, 'hits@1': 0.263656796638327, 'hits@10': 0.5430225740252126}
621 train loss: 3.7181349
622 train loss: 3.717974
623 train loss: 3.7187204
Epoch 00624: reducing learning rate of group 0 to 5.0332e-04.
624 train loss: 3.714197
625 train loss: 3.7129588
626 train loss: 3.7117805
627 train loss: 3.7096593
628 train loss: 3.7101254
629 train loss: 3.7095985
630 train loss: 3.7105997
valid {'mr': 147.79287140005704, 'mrr': 0.3595370822097406, 'hits@1': 0.2680923866552609, 'hits@10': 0.5424579412603364}
test {'mr': 164.81095475422651, 'mrr': 0.3563137870636443, 'hits@1': 0.2631926121372032, 'hits@10': 0.5428026971562592}
631 train loss: 3.7102048
632 train loss: 3.7085428
633 train loss: 3.7120166
634 train loss: 3.70864
635 train loss: 3.70882
636 train loss: 3.709648
637 train loss: 3.7090232
638 train loss: 3.7093804
Epoch 00639: reducing learning rate of group 0 to 4.0265e-04.
639 train loss: 3.7059798
640 train loss: 3.703877
valid {'mr': 147.67025948103793, 'mrr': 0.3594753227551052, 'hits@1': 0.26777872825777016, 'hits@10': 0.5423723980610208}
test {'mr': 164.6406234730773, 'mrr': 0.3571086987345088, 'hits@1': 0.2642919964819701, 'hits@10': 0.5430225740252126}
641 train loss: 3.7059455
642 train loss: 3.7015316
643 train loss: 3.7039967
644 train loss: 3.701163
645 train loss: 3.7028236
646 train loss: 3.701632
647 train loss: 3.702424
648 train loss: 3.7006772
649 train loss: 3.699609
650 train loss: 3.7000644
valid {'mr': 148.23607071571143, 'mrr': 0.35967385595881024, 'hits@1': 0.2685201026518392, 'hits@10': 0.5414884516680923}
test {'mr': 164.41094986807389, 'mrr': 0.3562780334343986, 'hits@1': 0.2634857812958077, 'hits@10': 0.542704974103391}
651 train loss: 3.6989722
652 train loss: 3.699072
653 train loss: 3.700967
654 train loss: 3.6986427
655 train loss: 3.700518
656 train loss: 3.7010684
657 train loss: 3.6980343
658 train loss: 3.696811
659 train loss: 3.6973338
660 train loss: 3.6985168
valid {'mr': 148.4059880239521, 'mrr': 0.35908401401308565, 'hits@1': 0.26706586826347306, 'hits@10': 0.5429426860564585}
test {'mr': 166.11870907847162, 'mrr': 0.35641216033701995, 'hits@1': 0.2631681813739861, 'hits@10': 0.5435356200527705}
661 train loss: 3.699284
662 train loss: 3.6967077
663 train loss: 3.6999633
664 train loss: 3.6979735
Epoch 00665: reducing learning rate of group 0 to 3.2212e-04.
665 train loss: 3.6947773
666 train loss: 3.6942313
667 train loss: 3.6946392
668 train loss: 3.6938965
669 train loss: 3.6927485
670 train loss: 3.6936796
valid {'mr': 148.49284288565727, 'mrr': 0.35863932789938124, 'hits@1': 0.2661819218705446, 'hits@10': 0.5424009124607927}
test {'mr': 165.82221733606957, 'mrr': 0.3565168880388424, 'hits@1': 0.2631681813739861, 'hits@10': 0.5429981432619955}
671 train loss: 3.693095
672 train loss: 3.6950161
673 train loss: 3.6901264
674 train loss: 3.6932096
675 train loss: 3.6913505
676 train loss: 3.6910982
677 train loss: 3.692746
678 train loss: 3.6909723
679 train loss: 3.690658
Epoch 00680: reducing learning rate of group 0 to 2.5770e-04.
680 train loss: 3.6887708
valid {'mr': 147.74154548046764, 'mrr': 0.3587483351280425, 'hits@1': 0.26646706586826346, 'hits@10': 0.5425434844596522}
test {'mr': 164.92453337242256, 'mrr': 0.35669621131887935, 'hits@1': 0.2639011042704974, 'hits@10': 0.542729404866608}
681 train loss: 3.6889653
682 train loss: 3.6871505
683 train loss: 3.6873736
684 train loss: 3.688653
685 train loss: 3.6890683
686 train loss: 3.6859698
687 train loss: 3.6843503
688 train loss: 3.6858475
689 train loss: 3.687661
690 train loss: 3.6870053
valid {'mr': 147.70002851439978, 'mrr': 0.3589661453292051, 'hits@1': 0.2666951810664385, 'hits@10': 0.5425434844596522}
test {'mr': 164.362112772403, 'mrr': 0.3561227119217507, 'hits@1': 0.26260627381999413, 'hits@10': 0.5441952506596306}
691 train loss: 3.6852255
692 train loss: 3.6851223
693 train loss: 3.686513
Epoch 00694: reducing learning rate of group 0 to 2.0616e-04.
694 train loss: 3.6836436
695 train loss: 3.6848402
696 train loss: 3.6827211
697 train loss: 3.682667
698 train loss: 3.6813426
699 train loss: 3.682027
700 train loss: 3.6827226
valid {'mr': 147.58856572569147, 'mrr': 0.35901417485405956, 'hits@1': 0.266837753065298, 'hits@10': 0.5430282292557742}
test {'mr': 165.19156161438482, 'mrr': 0.3565406777149631, 'hits@1': 0.2635590735854588, 'hits@10': 0.5443418352389329}
701 train loss: 3.6809204
702 train loss: 3.6840727
703 train loss: 3.6811435
704 train loss: 3.6799445
705 train loss: 3.683233
706 train loss: 3.6829884
707 train loss: 3.6840136
708 train loss: 3.680453
709 train loss: 3.679433
710 train loss: 3.681475
valid {'mr': 148.2520387795837, 'mrr': 0.35851854348590656, 'hits@1': 0.2654690618762475, 'hits@10': 0.5437696036498432}
test {'mr': 165.5316866998925, 'mrr': 0.35621498011051156, 'hits@1': 0.26272842763607934, 'hits@10': 0.5431935893677319}
711 train loss: 3.6803675
712 train loss: 3.680987
713 train loss: 3.6814537
714 train loss: 3.6794355
715 train loss: 3.680648
Epoch 00716: reducing learning rate of group 0 to 1.6493e-04.
716 train loss: 3.6777885
717 train loss: 3.6780531
718 train loss: 3.6785176
719 train loss: 3.677126
720 train loss: 3.678683
valid {'mr': 148.1428001140576, 'mrr': 0.35925384473904026, 'hits@1': 0.2670943826632449, 'hits@10': 0.5429997148560023}
test {'mr': 166.1304358448158, 'mrr': 0.35645811122034593, 'hits@1': 0.2633391967165054, 'hits@10': 0.542704974103391}
721 train loss: 3.6783974
722 train loss: 3.676585
723 train loss: 3.6767933
724 train loss: 3.6760654
725 train loss: 3.6770701
726 train loss: 3.677541
727 train loss: 3.6766932
728 train loss: 3.6763864
729 train loss: 3.6778908
730 train loss: 3.6749933
valid {'mr': 148.57661819218706, 'mrr': 0.3599435461735295, 'hits@1': 0.2680068434559453, 'hits@10': 0.543313373253493}
test {'mr': 166.70749047200235, 'mrr': 0.35695164936232704, 'hits@1': 0.2637545196911952, 'hits@10': 0.5434623277631193}
731 train loss: 3.674683
732 train loss: 3.6737196
733 train loss: 3.6768637
734 train loss: 3.676911
735 train loss: 3.6746716
736 train loss: 3.6735735
737 train loss: 3.6746328
738 train loss: 3.6753302
Epoch 00739: reducing learning rate of group 0 to 1.3194e-04.
739 train loss: 3.6758666
740 train loss: 3.6737964
valid {'mr': 148.62349586541202, 'mrr': 0.36003239705541673, 'hits@1': 0.2684630738522954, 'hits@10': 0.5441117764471058}
test {'mr': 165.96069090198378, 'mrr': 0.3565254424711871, 'hits@1': 0.26351021205902475, 'hits@10': 0.5442685429492817}
741 train loss: 3.6726437
742 train loss: 3.6748981
743 train loss: 3.6750712
744 train loss: 3.6725645
745 train loss: 3.673223
746 train loss: 3.672196
747 train loss: 3.6724026
748 train loss: 3.6731002
749 train loss: 3.67335
750 train loss: 3.673971
valid {'mr': 148.3749358426005, 'mrr': 0.3595497108688225, 'hits@1': 0.26766467065868266, 'hits@10': 0.5439692044482464}
test {'mr': 165.83726668621128, 'mrr': 0.35710511067753403, 'hits@1': 0.26424313495553603, 'hits@10': 0.5440730968435454}
751 train loss: 3.6728446
752 train loss: 3.672909
Epoch 00753: reducing learning rate of group 0 to 1.0555e-04.
753 train loss: 3.672905
754 train loss: 3.6713421
755 train loss: 3.6700585
756 train loss: 3.668589
757 train loss: 3.6696181
758 train loss: 3.6702108
759 train loss: 3.6726635
760 train loss: 3.6717074
valid {'mr': 147.983376104933, 'mrr': 0.35974706453601574, 'hits@1': 0.2670943826632449, 'hits@10': 0.5439121756487026}
test {'mr': 165.65173947034106, 'mrr': 0.35720161757142055, 'hits@1': 0.2636079351118929, 'hits@10': 0.5446594351607544}
761 train loss: 3.6712735
762 train loss: 3.6716938
Epoch 00763: reducing learning rate of group 0 to 8.4442e-05.
763 train loss: 3.6708744
764 train loss: 3.6717618
765 train loss: 3.6694233
766 train loss: 3.6699858
767 train loss: 3.6692066
768 train loss: 3.6690466
Epoch 00769: reducing learning rate of group 0 to 6.7554e-05.
769 train loss: 3.671591
770 train loss: 3.6678367
valid {'mr': 148.942001710864, 'mrr': 0.3594449395323908, 'hits@1': 0.26743655546050754, 'hits@10': 0.542828628457371}
test {'mr': 166.32407407407408, 'mrr': 0.3564067624240792, 'hits@1': 0.26285058145216456, 'hits@10': 0.5428026971562592}
771 train loss: 3.670589
772 train loss: 3.6692605
773 train loss: 3.668741
774 train loss: 3.668126
775 train loss: 3.6673677
776 train loss: 3.6698592
777 train loss: 3.669108
778 train loss: 3.6683037
779 train loss: 3.6667674
780 train loss: 3.6681154
valid {'mr': 148.27040205303678, 'mrr': 0.36025389155827964, 'hits@1': 0.2688622754491018, 'hits@10': 0.5431137724550898}
test {'mr': 165.782981530343, 'mrr': 0.3567176414457352, 'hits@1': 0.26368122740154404, 'hits@10': 0.5436577738688556}
781 train loss: 3.6678483
782 train loss: 3.6698437
783 train loss: 3.6660395
784 train loss: 3.6667368
785 train loss: 3.6684632
786 train loss: 3.6665757
787 train loss: 3.6692839
788 train loss: 3.6685982
789 train loss: 3.665862
Epoch 00790: reducing learning rate of group 0 to 5.4043e-05.
790 train loss: 3.6676214
valid {'mr': 148.96871970345023, 'mrr': 0.3598047332381937, 'hits@1': 0.2682064442543484, 'hits@10': 0.5427715996578272}
test {'mr': 167.0235023942148, 'mrr': 0.3570758281248852, 'hits@1': 0.2641454119026678, 'hits@10': 0.5442929737124987}
791 train loss: 3.6681206
792 train loss: 3.6670687
793 train loss: 3.6675928
794 train loss: 3.6653333
795 train loss: 3.6646106
796 train loss: 3.6681223
797 train loss: 3.6681309
798 train loss: 3.667249
799 train loss: 3.6663132
800 train loss: 3.6647007
valid {'mr': 148.74864556601082, 'mrr': 0.35931823932138246, 'hits@1': 0.2671514114627887, 'hits@10': 0.5437410892500713}
test {'mr': 166.46440437799276, 'mrr': 0.35704368508478973, 'hits@1': 0.26392553503371446, 'hits@10': 0.5439265122642432}
801 train loss: 3.66604
Epoch 00802: reducing learning rate of group 0 to 4.3235e-05.
802 train loss: 3.6647193
803 train loss: 3.6682165
804 train loss: 3.667667
805 train loss: 3.6663985
806 train loss: 3.6665082
807 train loss: 3.66559
Epoch 00808: reducing learning rate of group 0 to 3.4588e-05.
808 train loss: 3.664904
809 train loss: 3.6655369
810 train loss: 3.6637995
valid {'mr': 148.19244368406046, 'mrr': 0.35940666034063745, 'hits@1': 0.26732249786142004, 'hits@10': 0.5437696036498432}
test {'mr': 166.23619661878237, 'mrr': 0.3572028772888948, 'hits@1': 0.26426756571875304, 'hits@10': 0.5447571582136226}
811 train loss: 3.6643908
812 train loss: 3.663556
813 train loss: 3.6658807
814 train loss: 3.6630383
815 train loss: 3.6653516
816 train loss: 3.665386
817 train loss: 3.667321
818 train loss: 3.6643348
819 train loss: 3.664244
820 train loss: 3.6649868
Epoch 00821: reducing learning rate of group 0 to 2.7670e-05.
valid {'mr': 149.0435129740519, 'mrr': 0.359315634371588, 'hits@1': 0.2672939834616481, 'hits@10': 0.5443113772455089}
test {'mr': 166.767785595622, 'mrr': 0.3567694164981109, 'hits@1': 0.26351021205902475, 'hits@10': 0.544415127528584}
821 train loss: 3.6643872
822 train loss: 3.6624885
823 train loss: 3.6648004
824 train loss: 3.6652627
825 train loss: 3.6650252
826 train loss: 3.6642654
827 train loss: 3.6624472
828 train loss: 3.6649225
Epoch 00829: reducing learning rate of group 0 to 2.2136e-05.
829 train loss: 3.6654177
830 train loss: 3.6640823
valid {'mr': 148.15848303393213, 'mrr': 0.35990590367839254, 'hits@1': 0.26752209865982324, 'hits@10': 0.5439977188480183}
test {'mr': 165.8282761653474, 'mrr': 0.3570853651818028, 'hits@1': 0.26351021205902475, 'hits@10': 0.5447815889768396}
831 train loss: 3.6634514
832 train loss: 3.6627715
833 train loss: 3.6637485
834 train loss: 3.6638162
Epoch 00835: reducing learning rate of group 0 to 1.7709e-05.
835 train loss: 3.6635213
836 train loss: 3.6629937
837 train loss: 3.661733
838 train loss: 3.6622882
839 train loss: 3.664427
840 train loss: 3.6641011
valid {'mr': 148.62212717422298, 'mrr': 0.3592270061099375, 'hits@1': 0.266837753065298, 'hits@10': 0.5438266324493869}
test {'mr': 166.49601778559563, 'mrr': 0.3564665930673174, 'hits@1': 0.2634369197693736, 'hits@10': 0.5439753737906772}
841 train loss: 3.6630201
842 train loss: 3.6632955
843 train loss: 3.6637347
Epoch 00844: reducing learning rate of group 0 to 1.4167e-05.
844 train loss: 3.6627207
845 train loss: 3.6621122
846 train loss: 3.6647515
847 train loss: 3.6638143
848 train loss: 3.6631398
849 train loss: 3.6632845
Epoch 00850: reducing learning rate of group 0 to 1.1334e-05.
850 train loss: 3.663942
valid {'mr': 147.9197604790419, 'mrr': 0.3600314669020128, 'hits@1': 0.268063872255489, 'hits@10': 0.5429997148560023}
test {'mr': 165.7252027753347, 'mrr': 0.35736407003975657, 'hits@1': 0.2641698426658849, 'hits@10': 0.544415127528584}
851 train loss: 3.6632679
852 train loss: 3.6619742
853 train loss: 3.6612136
854 train loss: 3.6638174
855 train loss: 3.6624548
856 train loss: 3.6637497
857 train loss: 3.6638172
858 train loss: 3.6607714
859 train loss: 3.662452
860 train loss: 3.664375
valid {'mr': 148.62192757342459, 'mrr': 0.3599493865316567, 'hits@1': 0.26783575705731394, 'hits@10': 0.5443969204448247}
test {'mr': 166.2904329131242, 'mrr': 0.3570725564184367, 'hits@1': 0.2637300889279781, 'hits@10': 0.5444884198182351}
861 train loss: 3.662511
862 train loss: 3.663523
863 train loss: 3.6638558
864 train loss: 3.66223
Epoch 00865: reducing learning rate of group 0 to 1.0000e-05.
865 train loss: 3.6632917
866 train loss: 3.6621528
867 train loss: 3.6626914
868 train loss: 3.6627195
869 train loss: 3.6617424
870 train loss: 3.6630952
valid {'mr': 148.68705446250357, 'mrr': 0.3594443011770261, 'hits@1': 0.26723695466210434, 'hits@10': 0.5438266324493869}
test {'mr': 166.37721098407116, 'mrr': 0.3567406793947888, 'hits@1': 0.2633636274797225, 'hits@10': 0.544415127528584}
871 train loss: 3.6643872
872 train loss: 3.6625614
873 train loss: 3.6631951
874 train loss: 3.66307
875 train loss: 3.6627676
876 train loss: 3.6634681
877 train loss: 3.6629045
878 train loss: 3.6649294
879 train loss: 3.6616032
880 train loss: 3.6631556
valid {'mr': 148.4462218420302, 'mrr': 0.35908760248430965, 'hits@1': 0.2667807242657542, 'hits@10': 0.5436270316509837}
test {'mr': 166.07981530343008, 'mrr': 0.3567826339115554, 'hits@1': 0.2634613505325906, 'hits@10': 0.5445128505814522}
881 train loss: 3.6642036
882 train loss: 3.662638
883 train loss: 3.6616344
884 train loss: 3.6647832
885 train loss: 3.6624308
886 train loss: 3.6643004
887 train loss: 3.660953
888 train loss: 3.6625562
889 train loss: 3.6637378
890 train loss: 3.660232
valid {'mr': 148.0360136869119, 'mrr': 0.35978565753528563, 'hits@1': 0.2673510122611919, 'hits@10': 0.5443398916452808}
test {'mr': 165.5219143946057, 'mrr': 0.3571174036593593, 'hits@1': 0.26358350434867583, 'hits@10': 0.5446594351607544}
891 train loss: 3.6625829
892 train loss: 3.6629348
893 train loss: 3.6622052
894 train loss: 3.6606135
895 train loss: 3.6630757
896 train loss: 3.6611001
897 train loss: 3.660912
898 train loss: 3.6610985
899 train loss: 3.6629162
900 train loss: 3.6620002
valid {'mr': 149.18400342172797, 'mrr': 0.3594222229950503, 'hits@1': 0.2676076418591389, 'hits@10': 0.5431993156544055}
test {'mr': 166.5351070067429, 'mrr': 0.3569723153484699, 'hits@1': 0.2638766735072804, 'hits@10': 0.5437554969217239}
901 train loss: 3.661927
902 train loss: 3.662413
903 train loss: 3.6594176
904 train loss: 3.6633
905 train loss: 3.6633284
906 train loss: 3.6616716
907 train loss: 3.663208
908 train loss: 3.6636612
909 train loss: 3.6628034
910 train loss: 3.6615055
valid {'mr': 148.4949244368406, 'mrr': 0.359697534030938, 'hits@1': 0.26737952666096376, 'hits@10': 0.5450242372398061}
test {'mr': 166.10011726766345, 'mrr': 0.3565779984620909, 'hits@1': 0.26282615068894755, 'hits@10': 0.5453679272940487}
911 train loss: 3.6636536
912 train loss: 3.6632361
913 train loss: 3.6633384
914 train loss: 3.661485
915 train loss: 3.660672
916 train loss: 3.6615272
917 train loss: 3.6617332
918 train loss: 3.6622741
919 train loss: 3.6612136
920 train loss: 3.6646037
valid {'mr': 148.27564870259482, 'mrr': 0.35993761441491257, 'hits@1': 0.26757912745936696, 'hits@10': 0.5445109780439121}
test {'mr': 165.8980748558585, 'mrr': 0.3572716739599864, 'hits@1': 0.2634369197693736, 'hits@10': 0.5453923580572657}
921 train loss: 3.6630466
922 train loss: 3.6614676
923 train loss: 3.6627386
924 train loss: 3.6611297
925 train loss: 3.6633475
926 train loss: 3.6617281
927 train loss: 3.6612382
928 train loss: 3.663381
929 train loss: 3.662259
930 train loss: 3.6608973
valid {'mr': 148.00923866552608, 'mrr': 0.35968893568125293, 'hits@1': 0.26737952666096376, 'hits@10': 0.5448816652409467}
test {'mr': 165.53664614482557, 'mrr': 0.3569707934662165, 'hits@1': 0.26363236587510996, 'hits@10': 0.5448793120297079}
931 train loss: 3.6609824
932 train loss: 3.6631145
933 train loss: 3.6619868
934 train loss: 3.6629963
935 train loss: 3.6623585
936 train loss: 3.6600852
937 train loss: 3.6630855
938 train loss: 3.6615167
939 train loss: 3.6633704
Final result{'mr': 166.0411414052575, 'mrr': 0.35804114550603083, 'hits@1': 0.2650004886152643, 'hits@10': 0.545587804163002}